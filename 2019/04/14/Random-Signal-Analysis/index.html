<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/st-32x32.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/st-16x16.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="signal processing,">










<meta name="description" content="This is an updating note on random signal analysis.">
<meta name="keywords" content="signal processing">
<meta property="og:type" content="article">
<meta property="og:title" content="Random Signal Analysis">
<meta property="og:url" content="http://michaelxiu.com/2019/04/14/Random-Signal-Analysis/index.html">
<meta property="og:site_name" content="Michael Xiu">
<meta property="og:description" content="This is an updating note on random signal analysis.">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://michaelxiu.com/assets/1555224354383.png">
<meta property="og:image" content="http://michaelxiu.com/assets/1555225278092.png">
<meta property="og:image" content="http://michaelxiu.com/assets/1557196005851.png">
<meta property="og:image" content="http://michaelxiu.com/assets/1561431198657.png">
<meta property="og:image" content="http://michaelxiu.com/assets/1561431789698.png">
<meta property="og:image" content="http://michaelxiu.com/assets/1561432334694.png">
<meta property="og:image" content="http://michaelxiu.com/assets/1561432801456.png">
<meta property="og:image" content="http://michaelxiu.com/assets/1558406463923.png">
<meta property="og:image" content="http://michaelxiu.com/assets/1559615726536.png">
<meta property="og:image" content="http://michaelxiu.com/assets/OFDM.png">
<meta property="og:image" content="http://michaelxiu.com/assets/1560249863674.png">
<meta property="og:image" content="http://michaelxiu.com/assets/cminmax.png">
<meta property="og:updated_time" content="2019-06-26T11:12:55.069Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Random Signal Analysis">
<meta name="twitter:description" content="This is an updating note on random signal analysis.">
<meta name="twitter:image" content="http://michaelxiu.com/assets/1555224354383.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://michaelxiu.com/2019/04/14/Random-Signal-Analysis/">





  <title>Random Signal Analysis | Michael Xiu</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
	
	<a href="https://github.com/Michael-Xiu" class="github-corner" aria-label="My GitHub"><svg width="80" height="80" viewbox="0 0 250 250" style="fill:#CABCBC; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Michael Xiu</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://michaelxiu.com/2019/04/14/Random-Signal-Analysis/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Michael.Xiu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Michael Xiu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Random Signal Analysis</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-14T09:13:53+08:00">
                2019-04-14
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/note/" itemprop="url" rel="index">
                    <span itemprop="name">note</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              

              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  50 mins
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>This is an updating note on random signal analysis.</p>
<a id="more"></a>
<h1 id="Random-Signal-Basic"><a href="#Random-Signal-Basic" class="headerlink" title="Random Signal Basic"></a>Random Signal Basic</h1><hr>
<h2 id="Review"><a href="#Review" class="headerlink" title="Review"></a>Review</h2><p>At first, we should have a prior that to define a random variable, one tool is <strong>probability</strong>, and the other one is <strong>statistics (moment, cumulants)</strong>.</p>
<h3 id="Probability-distribution"><a href="#Probability-distribution" class="headerlink" title="Probability distribution"></a>Probability distribution</h3><p>In 1D, at first, we have random variable <strong>X</strong> .</p>
<p>Then we have to describe the relationship between <strong>X</strong> and its probability distribution.</p>
<ul>
<li><p><strong>Culminative Distribution Function (CDF)</strong>: $ F(x)=p(X \leq x)$</p>
</li>
<li><p><strong>Probability Density Function (PDF)</strong>: $f(x)=\frac{dF(x)}{dx}$</p>
</li>
</ul>
<p>In 2D, random variable <strong>X</strong> can be specified as <strong>(X,Y)</strong></p>
<ul>
<li><p><strong>CDF</strong>: $F_{XY}(x, y)=p(X \leq x, Y \leq y)$</p>
</li>
<li><p><strong>PDF</strong>: $ \frac{\partial^{2} F_{XY}(x,y)}{\partial x \partial y}​$</p>
</li>
<li><p><strong>Marginal Distribution</strong>: $f_{X}(x)=\int\limits_{-\infty}^{+\infty} f_{XY}(x,y)dy​$</p>
</li>
<li><p><strong>Conditional Distribution</strong>: $f_Y(y|x)=\frac{f_{XY}(x,y)}{f_X(x)}$</p>
</li>
</ul>
<h3 id="Moment-and-Cumulant"><a href="#Moment-and-Cumulant" class="headerlink" title="Moment and Cumulant"></a>Moment and Cumulant</h3><p>In 1D case:</p>
<ul>
<li><p>The <em>n</em>-th <strong>ordinary moment</strong>:  $m_n=E[X^n],\ n=1,2,…$</p>
<blockquote>
<p>The zeroth moment is the total probability (i.e. one), the first moment is the mean, the second central moment is the variance, the third standardized moment is the skewness, and the fourth standardized moment is the kurtosis. The mathematical concept is closely related to the concept of moment in physics.</p>
<p>For the second and higher moments, <strong>the central moment</strong> are usually used rather than the moments about zero, because they provide clearer information about the distribution’s shape.</p>
</blockquote>
</li>
<li><p>The <em>n</em>-th <strong>central moment</strong>: $\mu_{n}=E\{(X-E[X])^n\},\ n=1,2,…$</p>
</li>
</ul>
<p>The second central moment $μ_2$ is called the variance, and is usually denoted $σ_2$, where σ represents the standard deviation.<br>  The third and fourth central moments are used to define the standardized moments which are used to define skewness and kurtosis, respectively.</p>
<p>Ordinary moment vs Central moment:</p>
<ul>
<li>The <strong>1st ordinary moment</strong> is the mean. — Define concentration.</li>
<li>The <strong>2nd central moment</strong> is the variance. — Define dispersion.</li>
<li>The <strong>3rd central moment</strong> is a standardized moment. — Define skewness.</li>
<li>The <strong>4th central moment</strong> is a standardized moment. — Define kurtosis.</li>
</ul>
<p><img src="/assets/1555224354383.png" alt="1555224354383"></p>
<font face="Segoe Print" color="#D87093" size="4">class note: Moments of gaussian variables</font>

<blockquote>
<p> Higher moments (&gt;2) are used to describe the deviation from gaussian variables. Only <em>1~2-th</em> moment are required to analyze gaussian variables because higher moments are relevant to these lower moments.</p>
</blockquote>
<p>In 2D case:</p>
<ul>
<li>The <em>(n+k)</em>-th <strong>mixed ordinary moment</strong>:  $m_{nk}=E[X^nY^k]$; when n=1, k=1, called relevant moment $R_{XY}$ .</li>
<li>The <em>(n+k)</em>-th <strong>mixed central moment</strong>:  $\mu_{nk}=E\{(X-E[X])^n(Y-E[Y])^k\}$; when n=1, k=1, called covariance $C_{XY}$</li>
<li><strong>Correlation Coefficient</strong>: $r_{X,Y}=\frac{cov(X,Y)}{\sigma_{X}\sigma_{Y}}$, where $cov(X,Y)$ is the 2nd mixed central moment called covariance and $\sigma​$ is standard deviations. It is a measure of the linear correlation between two variables.<br><img src="/assets/1555225278092.png" alt="1555225278092"></li>
</ul>
<ul>
<li><strong>Cumulant</strong>: a concept close to moment. </li>
</ul>
<p>As for gaussian variables, higher moments are determined by lower moments so only 1~2nd moments are necessary. From the perspective of cumulants, when n&gt;2, they are zero in this case. Moreover, if the gaussian variables have zero mean, moments are even the same as cumulants. We can take it easy.</p>
<h3 id="Statistical-independence-and-irrelevance"><a href="#Statistical-independence-and-irrelevance" class="headerlink" title="Statistical independence and irrelevance"></a>Statistical independence and irrelevance</h3><ul>
<li><strong>Statistical Independence</strong>: <script type="math/tex; mode=display">
\textbf{X}\ and\ \textbf{Y}\ are\ statistically\ independent \Leftrightarrow f_{XY}(x,y)=f_{X}(x)\bullet f_{Y}(y)</script></li>
</ul>
<ul>
<li><strong>Irrelevance</strong> (In short of linear irrelevance): $r_{XY}=0​$. We gain $R_{XY}=E[XY]=E[X]E[Y]​$.</li>
</ul>
<p>Generally speaking, Statistical Independence $\Rightarrow​$ Irrelevance. As for gaussian random variables, Statistical Independence $\Leftrightarrow​$ Irrelevance.</p>
<ul>
<li><strong>Orthogonality</strong>: $R_{XY}=E[XY]=0$</li>
</ul>
<hr>
<h2 id="Characteristic-function"><a href="#Characteristic-function" class="headerlink" title="Characteristic function"></a>Characteristic function</h2><blockquote>
<p>In probability theory and statistics, the characteristic function of any real-valued random variable completely defines its probability distribution. If a random variable admits a probability density function, then the characteristic function is the Fourier transform of the probability density function. Thus it provides the basis of an alternative route to analytical results compared with working directly with probability density functions or cumulative distribution functions. There are particularly simple results for the characteristic functions of distributions defined by the weighted sums of random variables. </p>
</blockquote>
<ul>
<li><strong>Characteristic function</strong>: $\phi(\omega)=E[e^{j\omega X}]​$</li>
</ul>
<p>The characteristic function is the Fourier transform of the probability density function. (It is not exactly the same, because of $\pm j\omega X$.)</p>
<p>Another feature is that characteristic function is correspond to moment, so characteristic function is also named <strong>moment-generating function</strong>.</p>
<hr>
<h2 id="Distribution-model"><a href="#Distribution-model" class="headerlink" title="Distribution model"></a>Distribution model</h2><p>In this part, we have a brief introduction of ordinary distribution models in order to classify our problems into different models.</p>
<h3 id="Easy-ones"><a href="#Easy-ones" class="headerlink" title="Easy ones"></a>Easy ones</h3><ul>
<li><strong>Binomial distribution</strong></li>
</ul>
<p>To be or not to be, that is the question. We have only two choices.</p>
<ul>
<li><strong>Poisson distribution</strong></li>
</ul>
<p>No matter how long I have been waiting for the bus, how many buses will arrive in the coming 5 mins are a constant.</p>
<blockquote>
<p>It is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant rate and independently of the time since the last event. (<a href="https://en.wikipedia.org/wiki/Poisson_distribution" target="_blank" rel="noopener">Poisson distribution</a>)</p>
</blockquote>
<ul>
<li><strong>Uniform distribution</strong></li>
</ul>
<p>There is no bias within the probabilities of different choices.</p>
<h3 id="Gaussian-distribution"><a href="#Gaussian-distribution" class="headerlink" title="Gaussian distribution"></a>Gaussian distribution</h3><p>Detailed information can be found in “Probability theory”.</p>
<h3 id="chi-2-distribution"><a href="#chi-2-distribution" class="headerlink" title="$\chi^2$ distribution"></a>$\chi^2$ distribution</h3><p> $\chi^2$ distribution is also named  chi-squared distribution.</p>
<ul>
<li><strong>Central $\chi^2​$  distribution: $Y=\sum_{i=1}^{n}{X_{i}^2}​$</strong></li>
</ul>
<p>It has n degrees of freedom and is the distribution of a sum of the squares of n independent standard normal random variables (Mean=0, variance=1).</p>
<ul>
<li><strong>Noncentral $\chi^2​$  distribution: $Y=\sum_{i=1}^{n}{X_{i}^2}​$</strong></li>
</ul>
<p>It has n degrees of freedom and is the distribution of a sum of the squares of n independent random variables (Mean=$m_{i}$, variance=$\sigma^2$).</p>
<h3 id="Rayleigh-distribution"><a href="#Rayleigh-distribution" class="headerlink" title="Rayleigh distribution"></a>Rayleigh distribution</h3><ul>
<li><strong>Rayleigh distribution</strong>: $R=\sqrt{Y}=\sqrt{X^1+X^2}​$</li>
</ul>
<p>It is essentially a central $\chi ^2$ distribution with two degrees of freedom.</p>
<ul>
<li><strong>Generalized Rayleigh distribution</strong>: $R=\sqrt{Y}=\sqrt{\sum_{i=1}^{n}{X_{i}^2}}​$</li>
</ul>
<p>It is essentially a central $\chi ^2$ distribution with n degrees of freedom.</p>
<ul>
<li><strong>Rice distribution</strong>: $R=\sqrt{Y}​$</li>
</ul>
<p>It is essentially a noncentral $\chi ^2$ distribution with n degrees of freedom.</p>
<hr>
<h1 id="Stochastic-Process"><a href="#Stochastic-Process" class="headerlink" title="Stochastic Process"></a>Stochastic Process</h1><h2 id="From-random-variable-to-stochastic-process"><a href="#From-random-variable-to-stochastic-process" class="headerlink" title="From random variable to stochastic process"></a>From random variable to stochastic process</h2><h3 id="Stochastic-process-definition"><a href="#Stochastic-process-definition" class="headerlink" title="Stochastic process definition"></a>Stochastic process definition</h3><blockquote>
<p>Historically, the random variables were associated with or indexed by a set of numbers, usually viewed as points in time, giving the interpretation of a stochastic process representing numerical values of some system randomly changing over time, such as the growth of a bacterial population, an electrical current fluctuating due to thermal noise, or the movement of a gas molecule. (<a href="https://en.wikipedia.org/wiki/Stochastic_process" target="_blank" rel="noopener">Stochastic process</a>)</p>
</blockquote>
<p>In short, a stochastic process has two dimensions - <strong>Time</strong> and <strong>Value</strong>. At a specific time, the value is a random variable. This random variable may be determined by time $t$, which means at another specific time, the random variable may differ. </p>
<p>P.S. Probability distribution and numerical characteristics describe the random variable in the <em>Value</em> axis. There is no random variables along the <em>Time</em> axis but properties of random variables change along the <em>Time</em> axis. <em>1D</em> means one random variable and <em>2D</em> means two random variables in different times.</p>
<h3 id="Probability-distribution-1"><a href="#Probability-distribution-1" class="headerlink" title="Probability distribution"></a>Probability distribution</h3><p>In 1D <em>(one variable $x$ given two specific time $t$)</em>, </p>
<ul>
<li><p><strong>CDF</strong>: $ F_{X}(x_1,t_1)=P(X(t_1)\leq x_1)$</p>
</li>
<li><p><strong>PDF</strong>: $f_X(x_1,t_1)=\frac {\partial F_X(x_1,t_1)}{\partial x_1}​$</p>
</li>
</ul>
<p>We can easily find that this stochastic process is a function of <em>x</em> and <em>t</em>.</p>
<p>In 2D <em>(two variable $x$ given two specific time $t$)</em>, </p>
<ul>
<li><p><strong>CDF</strong>: $ F_{X}(x_1,x_2;t_1,t_2)=P\{(X(t_1)\leq x_1, X(t_2)\leq x_2)\}$</p>
</li>
<li><p><strong>PDF</strong>: $f_X(x_1,x_2;t_1,t_2)=\frac{\partial ^2  F_X(x_1,x_2;t_1,t_2)}{\partial x_1 \partial x_2}$</p>
</li>
</ul>
<p>It describes the relevance of two moments of the stochastic process, which is widely used in engineering.</p>
<h3 id="Numerical-characteristic"><a href="#Numerical-characteristic" class="headerlink" title="Numerical characteristic"></a>Numerical characteristic</h3><p>In 1D,</p>
<ul>
<li>Expectation: $m_X(t)=E[X(t)]=\int\limits_{-\infty}^{+\infty}x f_{X}(x,t)dx​$</li>
<li>Variance: $\sigma^2 _X(t)=D[X(t)]=\int\limits_{-\infty}^{+\infty}[x-m_X(t)]^2 f_{X}(x,t)dx​$</li>
</ul>
<p>The expectation and variance are both definite functions of time t.</p>
<p>In 2D,</p>
<ul>
<li>Autocorrelation: $R_X(t_1,t_2)=E[X(t_1)X(t_2)]=\int\limits_{-\infty}^{+\infty}\int\limits_{-\infty}^{+\infty}x_1x_2 f_X(x_1,x_2;t_1,t_2)dx_1dx_2$. </li>
<li>Cross-correlation: $R_{XY}(t_1,t_2)=E[X(t_1)Y(t_2)]=\int\limits_{-\infty}^{+\infty}\int\limits_{-\infty}^{+\infty}xy f_{XY}(x,y;t_1,t_2)dxdy$.</li>
</ul>
<p>It is the 2nd mixed ordinary moment of random process. And it describes the relationship of two times.</p>
<font face="Segoe Print" color="#D87093" size="4">class note: The usage of autocorrelation</font>

<blockquote>
<p>In communication system, autocorrelation of a communication channel can be used to determine whether this channel changes abruptly. If $R_X(t_1,t_2)​$ is small, it indicates that this channel changes slowly, vice versa.<br>Autocorrelation is a square function that describes a linear relationship. It instructs the design of <strong>Adaptive differential pulse-code modulation (ADPCM)</strong>.</p>
</blockquote>
<hr>
<h2 id="Stationary-process-and-ergodicity"><a href="#Stationary-process-and-ergodicity" class="headerlink" title="Stationary process and ergodicity"></a>Stationary process and ergodicity</h2><h3 id="Strictly-stationary-process"><a href="#Strictly-stationary-process" class="headerlink" title="Strictly stationary process"></a>Strictly stationary process</h3><script type="math/tex; mode=display">
f_X(x_1,x_2,...,x_n;t_1,t_2,...,t_n)=f_X(x_1,x_2,...,x_n;t_1+\tau,t_2+\tau,...,t_n+\tau)</script><p>This is the definition of n-th strictly stationary process and its PDF doesn’t vary with the time.</p>
<font face="Segoe Print" color="#D87093" size="4">class note:</font>

<blockquote>
<p>n here means the number of points.</p>
<p>Strictly stationary process $\Rightarrow$ Expectation and variance are irrelevant with t. </p>
<p>Expectation and variance are irrelevant with t. + Gaussian process (PDF determined by 1~2-nd moment) $\Rightarrow$ Strictly stationary process.</p>
</blockquote>
<h3 id="Weakly-stationary-process"><a href="#Weakly-stationary-process" class="headerlink" title="Weakly stationary process"></a>Weakly stationary process</h3><script type="math/tex; mode=display">
E[X(t)]=m_X(t)=m_X</script><script type="math/tex; mode=display">
E_X[t_1,t_2] = R_X(\tau)</script><script type="math/tex; mode=display">
E[X^2(t)]<\infty</script><p>Weakly stationary process is 2-nd stationary. In contrast, strictly stationary process is n-th stationary.</p>
<h3 id="Ergodicity"><a href="#Ergodicity" class="headerlink" title="Ergodicity"></a>Ergodicity</h3><ul>
<li><p>cond1: $\overline{X(t)} = \lim_{T \rightarrow \infty}\frac{1}{2T}\int\limits_{-T}^{T}X(t)dt\ \ = \ \ E[X(t)]=m_X​$</p>
</li>
<li><p>cond2: $\overline{X(t)X(t+\tau)} = \lim_{T \rightarrow \infty}\frac{1}{2T}\int\limits_{-T}^{T}X(t)X(t+\tau)dt\ \ = \ \ E[X(t)X(t+\tau)]=R_X(\tau)$</p>
</li>
<li><p>cond3: Weakly stationary process</p>
</li>
</ul>
<p>In total: <strong>Time average = Phase average</strong> (Phase refers to the <em>Value</em> axis)</p>
<blockquote>
<p>In probability theory, an ergodic dynamical system is one that, broadly speaking, has the same behavior averaged over time as averaged over the space of all the system’s states in its phase space. </p>
</blockquote>
<font face="Segoe Print" color="#D87093" size="4">class note:</font>

<blockquote>
<p>The premise of ergodicity is the process should be stationary [ergodicity $\rightarrow​$ stationary], or the mean we gain cannot reflect the reality since the mean varies with time.</p>
<p>A <strong>stationary and ergodic</strong> process $\rightarrow$ Time average $\rightarrow$  (1) PDF or (2) statistic $\rightarrow$ Relevance $\rightarrow$ ADPCM</p>
<p>A <strong>weakly and non-stationary</strong> process $\rightarrow$ Short Time Window (since the process is stationary in a short period of time and the difference between periods indicates the time-varying characteristic of the process) $\rightarrow$ Relevance within a short period $\rightarrow$ ADPCM</p>
<p>In this case, there is a conflict between <strong>time resolution</strong> (prefer shorter window) and <strong>frequency resolution</strong> (prefer longer window) named as <em>time-frequency symmetry</em>.</p>
<p>In engineering, we need tens of samples to gain the time average.</p>
</blockquote>
<hr>
<h2 id="Power-spectrum-density-PSD"><a href="#Power-spectrum-density-PSD" class="headerlink" title="Power spectrum density (PSD)"></a>Power spectrum density (PSD)</h2><h3 id="PSD-definition"><a href="#PSD-definition" class="headerlink" title="PSD definition"></a>PSD definition</h3><ul>
<li><p>PSD of random process: $S_X(\omega)=\lim_{T \rightarrow \infty}\frac{1}{2T}E[|X_T(\omega)|^2]$, $\omega$ is a random variable.</p>
</li>
<li><p>Average power of random process: $P=\frac{1}{2\pi}\int\limits_{-\infty}^{+\infty}S_X(\omega)d\omega​$, the average of all $\omega​$. </p>
</li>
</ul>
<p>Important conclusion: <strong>PSD of random process</strong> $\Leftrightarrow$ <strong>amplitude spectrum deterministic signal</strong>, containing no <strong>phase</strong> information.  </p>
<font face="Segoe Print" color="#D87093" size="4">class note: Speech coding</font>

<blockquote>
<p>2nd statistics ($\approx$ PSD / autocorrelation) contains no phase information.</p>
<p>Speech coding: </p>
<ul>
<li>Method1 - Waveform coders: The waveform we receive is the same as the original signal. Speed is about 16～64 kb/s. Time-domain: PCM, ADPCM. Frequency-domain: Sub-band coders, adaptive transform coders</li>
<li>Method2 - Vocoders: The vocoder examines speech by measuring how its spectral characteristics change over time. Since the vocoder process sends only the parameters of the vocal model over the communication link, instead of a point-by-point recreation of the waveform, the bandwidth required to transmit speech can be reduced significantly. Information about the instantaneous frequency of the original voice signal (as distinct from its spectral characteristic) is discarded; it was not important to preserve this for the purposes of the vocoder’s original use as an encryption aid. (<a href="https://en.wikipedia.org/wiki/Vocoder" target="_blank" rel="noopener">Vocoder</a>)</li>
</ul>
</blockquote>
<h3 id="PSD-and-autocorrelation"><a href="#PSD-and-autocorrelation" class="headerlink" title="PSD and autocorrelation"></a>PSD and autocorrelation</h3><p>PSD and autocorrelation are a Fourier Transform pair: </p>
<script type="math/tex; mode=display">
R_X(\tau)=\frac{1}{2\pi}\int\limits_{-\infty}^{+\infty}S_X(\omega)e^{j\omega\tau}d\omega</script><hr>
<h2 id="Gaussian-process-and-white-noise"><a href="#Gaussian-process-and-white-noise" class="headerlink" title="Gaussian process and white noise"></a>Gaussian process and white noise</h2><font face="Segoe Print" color="#D87093" size="4">class note: Why we focus on gaussian processes?</font>

<blockquote>
<p><strong>Central limit theorem</strong>  : In some situations, when independent random variables are added, their properly normalized sum tends toward a normal distribution even if the original variables themselves are not normally distributed. (<a href="https://en.wikipedia.org/wiki/Central_limit_theorem" target="_blank" rel="noopener">Central limit theorem</a>)</p>
<p>Difference between white and gaussian: </p>
<ul>
<li><p>Ref1: White noise can be Gaussian, or uniform or even Poisson. It seems to be that the key characteristic of white noise is that each measurement is completely independent of the measurements preceding it. Also, it must be flat on the frequency spectrum.</p>
</li>
<li><p>Ref2: White noise has equal intensity over the whole frequency domain (i.e. nearly flat curve). Gaussian refers to (time domain) sample distribution: each sample has a normal distribution with zero mean and finite variance.</p>
</li>
<li><p>Ref3: In particular, if each sample of white noise has a normal distribution with zero mean, the signal is said to be <strong>additive white Gaussian noise</strong> (shown as follow). Gaussianity refers to the probability distribution with respect to the value, in this context the probability of the signal falling within any particular range of amplitudes, while the term ‘white’ refers to the way the signal power is distributed (i.e., independently) over time or among frequencies.<br><img src="/assets/1557196005851.png" alt="1557196005851"></p>
</li>
</ul>
</blockquote>
<h3 id="Gaussian-process"><a href="#Gaussian-process" class="headerlink" title="Gaussian process"></a>Gaussian process</h3><ul>
<li>A weakly stationary gaussian process is a strictly stationary process.</li>
<li>If $t_i$ and $t_j$ are irrelevant, they are independent at the same time.</li>
</ul>
<h3 id="White-noise"><a href="#White-noise" class="headerlink" title="White noise"></a>White noise</h3><p><strong>Definition</strong>: If random process $N(t)$ has an zero mean and its  PSD is a constant as follow:</p>
<script type="math/tex; mode=display">
S_N(\omega)=\frac{N_0}{2},\ -\infty<\omega<\infty</script><p>The autocorrelation of white noise has this form: </p>
<script type="math/tex; mode=display">
R_N(\tau)=\frac{N_0}{2}\delta(\tau)</script><font face="Segoe Print" color="#D87093" size="4">class note:</font>

<blockquote>
<ul>
<li>The definition of white noise here is specific for 2nd white noise.</li>
<li>In reality,  signals of narrowband system can be similar to white noise. However, in broadband system, this assumption fails.</li>
<li><p>How <strong>PDF of white noise $\Rightarrow$ PDF</strong> :</p>
<ul>
<li><p>Gaussian white noise: [ PSD $\rightarrow$ autocorrelation (2nd statistic) $+$ zero mean (gaussian noise’s attribute) (1nd statistic) ] $\rightarrow$ definite gaussian variable $\rightarrow​$ PDF WITHOUT specific time value (The character of random signal processing)</p>
</li>
<li><p>NOT Gaussian white noise: it lacks &gt;2nd statistics.</p>
</li>
</ul>
</li>
</ul>
</blockquote>
<hr>
<h1 id="System-response"><a href="#System-response" class="headerlink" title="System response"></a>System response</h1><h2 id="PDF"><a href="#PDF" class="headerlink" title="PDF"></a>PDF</h2><h3 id="1-gaussian-Input"><a href="#1-gaussian-Input" class="headerlink" title="1. gaussian Input"></a>1. gaussian Input</h3><p>Gaussian input, gaussian output.</p>
<h3 id="2-non-gaussian-input"><a href="#2-non-gaussian-input" class="headerlink" title="2. non-gaussian input"></a>2. non-gaussian input</h3><ul>
<li>If PSD of $x(t) &gt;&gt;​$ bandwidth, $f_Y(x,t)​$ is close to gaussian distribution.</li>
<li>If PSD of $x(t)&lt;&lt;$ bandwidth, the distortion is slight and $f_Y(x,t)$ is close to $f_X(x,t)$.</li>
<li>In practical use, if PSD $\Delta f_X &gt; 7 \Delta f_H$ ($\Delta f_H$ is channel bandwidth), $f_Y(x,t)$ is regarded as gaussian distribution.</li>
</ul>
<h2 id="Expectation-and-autocorrelation"><a href="#Expectation-and-autocorrelation" class="headerlink" title="Expectation and autocorrelation"></a>Expectation and autocorrelation</h2><ul>
<li>Expectation: $E[Y(t)] = m_X\int\limits_{-\infty}^{+\infty}h(\tau)d\tau = m_Y $</li>
<li>Autocorrelation: $R_Y(\tau)=R_X(\tau)\ast h(-\tau) \ast h(\tau)$ [Tips: $h(\tau)$ is from output so autocorrelation (2nd) of Y has 2 $h(\tau)​$]</li>
<li>Cross-correlation: $R_{XY}(\tau)=R_X(\tau) \ast h(\tau)​$</li>
</ul>
<font face="Segoe Print" color="#D87093" size="4">class note:</font>

<blockquote>
<ul>
<li>Stationary $\rightarrow$ Time invariant $\rightarrow$ Stationary</li>
<li>Not stationary $\rightarrow$ Time invariant $\rightarrow$ Not stationary</li>
<li>Stationary $\rightarrow$ Time variant $\rightarrow$ Not stationary</li>
</ul>
</blockquote>
<h2 id="PSD"><a href="#PSD" class="headerlink" title="PSD"></a>PSD</h2><ul>
<li>PSD: $S_Y(\omega)=S_X(\omega)H(-\omega)H(\omega)=S_X(\omega)|H(\omega)|^2$</li>
</ul>
<p><br></p>
<font face="Segoe Print" color="#D87093" size="4">class note: Channel Estimation</font>  

<p><br><font face="Segoe Print" color="#4CABEE" size="3">Wired Channel:</font> It is relevant to transmission bandwidth and signal bandwidth. If we know $h(t)$, we can design a reverse system for equalization.</p>
<p><br><font face="Segoe Print" color="#4CABEE" size="3">Wireless Channel:</font> It is irrelevant to bandwidth and allows a wide bandwidth. It is relevant to multipath propagation which will bring ISI in a high speed. Here there are 3 methods to estimate this channel:</p>
<ul>
<li><strong>Training sequence</strong> (pilot sequence): [know input &amp; output | Cross-correlation] We know $R_{XY}(\tau)$ and $R_X(\tau)$ of $R_{XY}(\tau)=R_X(\tau)*h(\tau)$, it is easy to gain $h(\tau)​$. <a href="https://en.wikipedia.org/wiki/Channel_state_information" target="_blank" rel="noopener">Channel state information</a> </li>
</ul>
<p><br><font face="Segoe Print" color="#FFC300" size="3">  White Assumption:</font> </p>
<ul>
<li><p><strong>Blind channel estimation</strong>: [know output | Cross-correlation] Assuming that $x(t)$ is <em>i.i.d.</em> and 2nd white. Thus, $R_X=\delta(t)$. Based on the definition of cross-correlation, we can estimate the $|\hat{H}(\omega)| = c |H(\omega)|$, which is only the amplitude spectrum.</p>
</li>
<li><p><strong>Model based estimation</strong>: [know output | PSD] Assuming that $x(t)$ is <em>i.i.d.</em> and 2nd white. Then we have  $S_X(\omega)$ and $S_Y(\omega)$ which is short in most cases. There are several models to gain long but varied $|\hat{H}(\omega)|$</p>
</li>
</ul>
<font face="Segoe Print" color="#FFC300" size="3">  LTI system models for random signals – AR, MA and ARMA models：</font> 

<p><a href="https://www.gaussianwaves.com/2014/05/" target="_blank" rel="noopener">source</a></p>
<p>Motivation: We want to describe the PSD of a signal $x[n]​$ with the following model. In this way we can describe the signal $x[n]​$ only with the parameters of the LTI system and thus reduce the burden of communication system.</p>
<p><img src="/assets/1561431198657.png" alt="1561431198657"></p>
<ul>
<li><strong>Autoregressive model (<a href="https://en.wikipedia.org/wiki/Autoregressive_model" target="_blank" rel="noopener">AR</a>) </strong>: Sum of past $x[n-k]$ + current source input $\omega [n]$</li>
</ul>
<script type="math/tex; mode=display">
x[n]+a_1x[n-1]+a_2x[n-2]+...+a_Nx[n-N]=\omega[n]</script><p>where $a_i, … ,a_N​$ are the parameters of the model and $\omega [n]​$ is the source input white noise. </p>
<p>The frequency response of the IIR filter is well known: <strong><em>$H(e^{jω})​$ is an all-poles system</em></strong></p>
<script type="math/tex; mode=display">
H(e^{j\omega})=\frac{1}{\sum_{k=0}^{N}a_ke^{-jk\omega}}, \ a_0=1</script><p>The PSD of $x[n]$ is:</p>
<p><img src="/assets/1561431789698.png" alt="1561431789698"></p>
<ul>
<li><strong>Moving-average model (<a href="https://en.wikipedia.org/wiki/Moving-average_model" target="_blank" rel="noopener">MA</a>) </strong>: Sum of past and current source inputs <em>i.i.d.</em> $\omega[n-k]$ <script type="math/tex; mode=display">
x[n]= b_0\omega[n]+b_1\omega[n-1]+b_2\omega[n-2]+...+b_M\omega[n-M]</script>where the $b_0,…,b_M$ are the parameters of the model and the $\omega[n], \omega[n-1], …, \omega[n-M]​$ are <em>i.i.d.</em> source inputs.  <a href="https://www.youtube.com/watch?v=lUhtcP2SUsg" target="_blank" rel="noopener">video</a></li>
</ul>
<p>The frequency response of the FIR filter is well known: <strong><em>$H(e^{jω})​$ is an all-zeros system</em></strong></p>
<script type="math/tex; mode=display">
H(e^{j\omega})=\sum_{k=0}^{M}b_k e^{-jk\omega}</script><p>The PSD of $x[n]$ is:</p>
<p><img src="/assets/1561432334694.png" alt="1561432334694"></p>
<ul>
<li><strong>Autoregressive–moving-average model (<a href="https://en.wikipedia.org/wiki/Autoregressive%E2%80%93moving-average_model" target="_blank" rel="noopener">ARMA</a>)</strong> : <script type="math/tex; mode=display">
x[n]+a_1x[n-1]+...+a_Nx[n-N] = b_0\omega[n]+b_1\omega[n-1]+...+b_M\omega[n-M]</script></li>
</ul>
<p>The frequency response of this generalized filter is well known: <strong><em>$H(e^{jω})$ is an pole-zero system</em></strong></p>
<script type="math/tex; mode=display">
H(e^{j\omega})=\frac{\sum_{k=0}^{M}b_k e^{-jk\omega}}{\sum_{k=0}^{N}a_ke^{-jk\omega}}, \ a_0=1</script><p>The PSD of $x[n]​$ is:</p>
<p><img src="/assets/1561432801456.png" alt="1561432801456"></p>
<hr>
<h1 id="Introduction-of-Estimation-Theory"><a href="#Introduction-of-Estimation-Theory" class="headerlink" title="Introduction of Estimation Theory"></a>Introduction of Estimation Theory</h1><h2 id="Estimation-in-signal-processing"><a href="#Estimation-in-signal-processing" class="headerlink" title="Estimation in signal processing"></a>Estimation in signal processing</h2><p>Mathematically, we have the $N$-point data set ${x[0], x[1], … , x[N-1]}$ which depends on an unknown parameter $\theta$. We wish to determine $\theta$ based on the data or to define an estimator </p>
<script type="math/tex; mode=display">
\hat{\theta}=g({x[0], x[1], ... , x[N-1]})</script><p>where $g$ is some function. This is the problem of <strong><em>parameter estimation</em></strong>.</p>
<font face="Segoe Print" color="#D87093" size="4">class note: what is $\hat{\theta}?​$</font>  

<blockquote>
<p> Since $X​$ is a random variable, the estimation $\hat{\theta}​$ is also a random variable. However, the time series ${x[0], x[1], … , x[N-1]}​$ is a sample of $X​$ and thus $\hat{\theta}​$ gained from the $g​$ is a deterministic estimation. <strong>It is pretty important to bear in mind that every estimation based on specific input-output is only a sample.</strong></p>
 <font face="Segoe Print" color="#5E5CC" size="4">If we can improve the performance of random variable $\boldsymbol{\hat{\theta}}$, when we dealing with deterministic signals, we are more likely to gain a better deterministic estimation.</font>  





</blockquote>
<h2 id="The-mathematical-estimation-problem"><a href="#The-mathematical-estimation-problem" class="headerlink" title="The mathematical estimation problem"></a>The mathematical estimation problem</h2><p>In determining good estimators, the first step is to mathematically model the data. Because the data are inherently random, we describe it by its <strong>probability density function (PDF)</strong> or $\mathbf{p(x[0], x[1], … , x[N-1];\boldsymbol\theta)}​$</p>
<p>If we have prior knowledge <strong><em>Bayesian Estimation</em></strong> is a probable method and we describe the data by <em>joint PDF</em>:</p>
<script type="math/tex; mode=display">
p(\mathbf{x},\theta)=p(\mathbf{x}|\theta)p(\theta)</script><p>where $p(\theta)$ is the prior PDF, summarizing our knowledge about $\theta$ before any data are observed, and $p(x|\theta)$ is a conditional PDF, summarizing our knowledge provided by the data $\mathbf{x}$ conditioned on knowing $\theta$.</p>
<font face="Segoe Print" color="#D87093" size="4">class note: the notation of probability $f(x;\theta)$, $f(x,\theta)$ and $f(x|\theta)$ </font>  

<blockquote>
<p>$f(x;\theta)$ is the density of the random variable $X$ at the point $x$, with $\theta$ being the parameter of the distribution. $f(x,\theta)$ is the joint density of $X$ and $\Theta$ at the point $(x,\theta)$ and only makes sense if $\Theta$ is a random variable. $f(x|\theta)$ is the conditional distribution of $X$ given $\Theta$, and again, only makes sense if $\Theta$ is a random variable. This will become much clearer when you get further into the book and look at Bayesian analysis.</p>
<p>$f(x; \theta) \approx f(x,\theta)$ in this course.</p>
</blockquote>
<hr>
<h1 id="Minimum-Variance-Unbiased-Estimation"><a href="#Minimum-Variance-Unbiased-Estimation" class="headerlink" title="Minimum Variance Unbiased Estimation"></a>Minimum Variance Unbiased Estimation</h1><p><strong>MVU</strong> is a standard of an effective estimation. </p>
<p>It includes two parts: <em>Unbiased</em> and <em>Minimum Variance</em></p>
<p>It is the goal of our methods for estimation.</p>
<p>Specific methods to reach to MVU are presented in latter chapters.</p>
<font face="Segoe Print" color="#D87093" size="4">class note: Why mean+variance is sufficient?</font>

<blockquote>
<p>This is because $\hat{\theta}$ is often generated by linear equations, with <strong>Central limit theorem</strong>, $\hat{\theta}$ is similar to a gaussian variable. This is the reason why we only need <em>mean</em>, 1st statistic, and <em>variance</em>, 2nd statistic, to evaluate $\hat{\theta}$.</p>
</blockquote>
<h2 id="Unbiased-estimation"><a href="#Unbiased-estimation" class="headerlink" title="Unbiased estimation"></a>Unbiased estimation</h2><p>Since the parameter value may in general be anywhere in the interval $a &lt; \theta &lt; b$, unbiasedness asserts that no matter what the true value of $\theta $, our estimator will yield it on the average. </p>
<script type="math/tex; mode=display">
E(\hat{\theta}) = \theta \ \ \ \ a<\theta<b</script><h2 id="Minimum-Variance-Criterion"><a href="#Minimum-Variance-Criterion" class="headerlink" title="Minimum Variance Criterion"></a>Minimum Variance Criterion</h2><p>In searching for optimal estimators we need to adopt some optimality criterion. A natural one is the <strong><em>mean square error</em> (MSE)</strong>:</p>
<script type="math/tex; mode=display">
mse(\hat{\theta})=E[(\hat{\theta}-\theta)^2]</script><p>This measures the average mean squared deviation of the estimator from the true value. Unfortunate, adoption of this natural criterion leads to unrealizable estimators, and so we rewrite the MSE as</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
mse(\hat{\theta})&=E\{[(\hat{\theta}-E(\hat{\theta}))+(E(\hat{\theta})-\theta)]^2\}\\
&= var(\hat{\theta})+[E(\hat{\theta})-\theta)]^2 \\
&= var(\hat{\theta}) + 0 \qquad \qquad  \qquad  \qquad  \qquad   if\ unbiased
\end{aligned}
\end{equation}</script><p><strong>Minimum Variance Unbiased Estimation</strong> is to constrain the bias to be zero and find the estimator which minimizes the variance.</p>
<hr>
<h1 id="Cramer-Rao-Lower-Bound"><a href="#Cramer-Rao-Lower-Bound" class="headerlink" title="Cramer-Rao Lower Bound"></a>Cramer-Rao Lower Bound</h1><font face="Segoe Print" color="#D87093" size="4">class note: why we need CRLB?</font>  

<blockquote>
<p> From the last chapter, we know that a better estimation will have lower variance. CRLB provides the lower band of this variance and it becomes the objective of designs of algorithms.</p>
</blockquote>
<h2 id="Efficiency"><a href="#Efficiency" class="headerlink" title="Efficiency"></a>Efficiency</h2><p>An estimator which is unbiased and attains the CRLB is said to be efficient in that it efficiently uses the data. An MVU estimator may or may not be efficient.</p>
<h2 id="Scalar-parameter"><a href="#Scalar-parameter" class="headerlink" title="Scalar parameter"></a>Scalar parameter</h2><p><strong>Theorem (Cramer-Rao Lower Bound - Scalar Parameter)</strong></p>
<p>1) </p>
<p><em>It is assumed that the PDF $p(\mathbf{x};\theta)​$ satisfied the “regularity” condition</em></p>
<script type="math/tex; mode=display">
E[\frac{\partial \ \ln p(\mathbf{x};\theta)}{\partial \theta}]=0 \ \ \ \ for\ all\ \theta</script><p><em>where the expectation is taken with respect to $p(\mathbf{x};\theta)$. Then, the variance of any unbiased estimator $\hat{\theta}$ must satisfy</em></p>
<script type="math/tex; mode=display">
var(\hat{\theta})\geq \frac{1}{-E[\frac{\partial^2 \ \ln p(\mathbf{x};\theta)}{\partial \theta^2}]}</script><p><em>where the derivative is evaluated at the true value of $\theta$ and the expectation is taken with respect to $p(\mathbf{x};\theta)$.</em></p>
<p>2)</p>
<p>The denominator is referred to as the <strong><em>Fisher information</em></strong>:</p>
<script type="math/tex; mode=display">
I(\theta)=-E[\frac{\partial^2 \ \ln p(\mathbf{x};\theta)}{\partial \theta^2}]</script><p>The more information, the lower the bound.</p>
<p>3)</p>
<p><em>Furthermore, an unbiased estimator may be found that attains the bound for all $\theta$ if and only if</em> </p>
<script type="math/tex; mode=display">
\frac{\partial \ \ln p(\mathbf{x};\theta)}{\partial \theta}=I(\theta)(g(\mathbf{x})-\theta))</script><p><em>for some functions $g$ and $I$. That estimator, which is the MVU estimator, is $\hat{\theta}=g(\mathbf{x})$, and the minimum variance is $1/I(\theta)$.</em></p>
<p><img src="/assets/1558406463923.png" alt="1558406463923"></p>
<h2 id="Scalar-parameter-·-transformation"><a href="#Scalar-parameter-·-transformation" class="headerlink" title="Scalar parameter · transformation"></a>Scalar parameter · transformation</h2><p><strong>Situation: The parameter we wish to estimate is a function of some more fundamental parameter.</strong></p>
<p>Although efficiency is reserved only over linear transformations, it is approximately maintained over nonlinear transformations <strong>if the data record is large enough (Asymptotic efficiency)</strong>. This has great practical significance in that we are frequently interested in estimating functions of parameters.</p>
<p>Given the transformation: $g(\theta)=a\theta +b$</p>
<ul>
<li><p>Step1: Estimate single parameter $\hat{\theta}$.</p>
</li>
<li><p>Step2:  $\widehat{g(\theta)}=g(\hat{\theta})=a\hat{\theta}+b$</p>
</li>
</ul>
<h2 id="Vector-parameter"><a href="#Vector-parameter" class="headerlink" title="Vector parameter"></a>Vector parameter</h2><p><strong>Theorem (Cramer-Rao Lower Bound - Vector Parameter)</strong></p>
<p>1) </p>
<p><em>It is assumed that the PDF $p(\mathbf{x}; \boldsymbol \theta)​$ satisfies the “regularity” conditions:</em></p>
<script type="math/tex; mode=display">
E[\frac{\partial \ \ln p(\mathbf{x};\boldsymbol \theta)}{\partial \boldsymbol \theta}]=0 \ \ \ \ for\ all\ \boldsymbol \theta</script><p><em>where the expectation is taken with respect to $p(\mathbf{x}; \boldsymbol \theta)​$. Then, the covariance matrix of any unbiased estimator $\boldsymbol{ \hat{\theta}} ​$ satisfies:</em></p>
<script type="math/tex; mode=display">
\mathbf{C_{\boldsymbol{ \hat{\theta}}}}-\mathbf{I}^{-1}(\boldsymbol {\theta})\geq\mathbf 0</script><p><em>where $\geq\mathbf 0$ is interpreted as meaning that the matrix is positive semidefinite.</em></p>
<blockquote>
<p>In probability theory and statistics, a <strong>covariance matrix</strong> is a matrix whose element in the <em>i</em>,<em>j</em> position is the covariance between the <em>i</em>-th and <em>j</em>-th elements of a random vector. A random vector is a random variable with multiple dimensions. Each element of the vector is a scalar random variable.</p>
<p>A $n \times n$ symmetric real matrix $\displaystyle M$ is said to be <strong>positive semidefinite</strong> or non-negative definite if ${\displaystyle x^{\textsf {T}}Mx\geq 0}$  for all non-zero ${\displaystyle x}$ in ${\displaystyle \mathbb {R} ^{n}}$ . Formally,</p>
<script type="math/tex; mode=display">
M\ positive\ semidefinite \Leftrightarrow {\displaystyle x^{\textsf {T}}Mx\geq 0} \ for \ all \ x \in{\displaystyle \mathbb {R} ^{n}}</script></blockquote>
<p>2)</p>
<p>$\mathbf{I}(\boldsymbol\theta)$ is the $p\times p​$ <strong><em>Fisher information matrix</em></strong>. The latter is defined by:</p>
<script type="math/tex; mode=display">
[\mathbf{I}(\boldsymbol\theta)]_{ij}=-E[\frac{\partial^2 \ \ln p(\mathbf{x};\boldsymbol\theta)}{\partial \theta_{i}\partial \theta_j}] \ \ \ i=1,2,...,p;\ j=1,2,...,p</script><blockquote>
<p>$\mathbf{I}^{-1}$ exists $\Leftrightarrow$ $rank(\mathbf{I})=p$<br>If $rank(\mathbf{I})\prec p$, we can’t estimate an efficient channel. <em>e.g.</em> If $rank(\mathbf{I})=p-1$, we require a known parameter.</p>
</blockquote>
<p>3)</p>
<p><em>Furthermore, an unbiased estimator may be found that attains the bound in that $\mathbf{C_{\boldsymbol{ \hat{\theta}}}}=\mathbf{I}^{-1}(\boldsymbol {\theta})​$ if and only if</em></p>
<script type="math/tex; mode=display">
\frac{\partial \ \ln p(\mathbf{x};\boldsymbol \theta)}{\partial \boldsymbol \theta}=\mathbf{I}(\boldsymbol {\theta})(\mathbf g(\mathbf{x})-\boldsymbol \theta))</script><p><em>for some p-dimensional function $\mathbf g$ and some $p \times p$ matrix $\mathbf I$. That estimator, which is the MVU estimator, is $\boldsymbol {\hat {\theta}} = \mathbf g(\mathbf{x})$, and its covariance matrix is $\mathbf{I}^{-1}(\boldsymbol {\theta})$.</em></p>
<h2 id="Vector-parameter-·-transformation"><a href="#Vector-parameter-·-transformation" class="headerlink" title="Vector parameter · transformation"></a>Vector parameter · transformation</h2><p>Similar to “Scalar parameter · transformation”.</p>
<hr>
<h1 id="Linear-Model"><a href="#Linear-Model" class="headerlink" title="Linear Model"></a>Linear Model</h1><font face="Segoe Print" color="#D87093" size="4">class note: why we need Linear Model?</font>  

<blockquote>
<p>The determination of the MVU estimator is in general a difficult task. It is fortunate, however, that a large number of signal processing estimation problems can be represented by a data model that allows us to easily determine this estimator. This class of models is the <strong>linear model</strong>.</p>
</blockquote>
<h2 id="Linear-model-·-specific"><a href="#Linear-model-·-specific" class="headerlink" title="Linear model · specific"></a>Linear model · specific</h2><p><strong>Theorem (Minimum Variance Unbiased Estimator for the Linear Model)</strong></p>
<p><em>If the data observed can be modeled as</em></p>
<script type="math/tex; mode=display">
\mathbf x = \mathbf H \boldsymbol \theta + \mathbf w</script><p><em>where $\mathbf{x}$ is an $N \times 1$ vector of observations, $\mathbf H$ is a known $N\times p$ observation matrix (with $N\succ p$) and rank p, $\boldsymbol \theta$ is a $p\times1$ vector of parameters to be estimated, and $\mathbf w$ is an $N\times1$ noise vector with PDF $N(\mathbf 0, \sigma^2 \mathbf I)$, then the MVU estimator is</em></p>
<script type="math/tex; mode=display">
\boldsymbol{\hat{\theta}}=(\mathbf{H}^T\mathbf{H})^{-1}\mathbf{H}^T\mathbf x</script><p><em>and the covariance matrix of $\boldsymbol{\hat{\theta}}$ is</em></p>
<script type="math/tex; mode=display">
\mathbf{C_{\boldsymbol{\hat{\theta}}}}=\sigma^2(\mathbf{H}^T\mathbf{H})^{-1}.</script><p><em>For the linear model the MVU estimator is efficient in that it attains the CRLB.</em></p>
<h2 id="Linear-model-·-general"><a href="#Linear-model-·-general" class="headerlink" title="Linear model · general"></a>Linear model · general</h2><p>A more general form of the linear model allows for noise that is not white. The general linear model assumes that</p>
<script type="math/tex; mode=display">
\mathbf w \sim N(\mathbf 0, \mathbf C)</script><p>where $\mathbf C$ is not necessarily a scaled identity matrix. The MVU estimator is</p>
<script type="math/tex; mode=display">
\boldsymbol{\hat{\theta}}=(\mathbf{H}^T \mathbf C^{-1} \mathbf{H})^{-1}\mathbf{H}^T \mathbf C^{-1} \mathbf x</script><p>The biggest difference with “Linear model · white noise” is that the estimator requires statistics of the noise. The covariance matrix of $\boldsymbol{\hat{\theta}}$ is</p>
<script type="math/tex; mode=display">
\mathbf{C_{\boldsymbol{\hat{\theta}}}}=(\mathbf{H}^T \mathbf C^{-1} \mathbf{H})^{-1}.</script><font face="Segoe Print" color="#D87093" size="4">class note: Serial channel estimation using linear model</font>  

<p>This method is based on <strong>Training sequence</strong>.</p>
<p><img src="/assets/1559615726536.png" alt="1559615726536"></p>
<p>The receive signals start at $t+L$ because we should make sure that our $y$ is totally derived from prior signal instead of data.</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
y(t+L) &= \sum_{l=0}^{L-1}{h_{l}}s(t+L-l)+n\\
\vdots \\
y(t+L+Q) &= \sum_{l=0}^{L-1}{h_{l}}s(t+L+Q-l)+n
\end{aligned}
\end{equation}</script><p>$L​$ is the channel latency, $S​$ is the prior signal with length of $Q​$.  Then we can build a linear model to gain the MVU estimator:</p>
<script type="math/tex; mode=display">
\mathbf{Y}=
\left[
 \begin{matrix}
S
  \end{matrix} 
\right]
\left[
 \begin{matrix}
h_o\\
\vdots \\
h_{L-1}
  \end{matrix} 
\right]
+ \mathbf{N}

\Rightarrow

\mathbf{Y}=\mathbf{H}\boldsymbol{\theta}+\mathbf{N}</script><p>$\left[<br> \begin{matrix}<br>S<br>  \end{matrix}<br>\right]​$ is a matrix with $s(t)​$ components.</p>
<p>There are three rules:</p>
<ul>
<li>The length of prior signal $\geq​$ $2L-1​$. To enhance performance, the length should increase.</li>
<li><p>$\mathbf{H}$ requires to be <em>full column rank</em> and thus we should better design the prior signal $s$.</p>
</li>
<li><p>$\mathbf{H}^T\mathbf{H}=C \cdot \mathbf{I}​$ is the best choice because $\mathbf{H}^T\mathbf{H}​$ has uniform eigenvalues so its inverse will not enlarge the $\mathbf{N}​$.</p>
</li>
</ul>
<font face="Segoe Print" color="#D87093" size="4">class note: Parellel channel estimation using linear model</font>

<p>In an OFDM system, we can estimate its channel based on pilot sequence in frequency domain and linear model. The length of signals in each step are shown as follow:</p>
<p><img src="/assets/OFDM.png" alt="1560612568718"></p>
<p>The whole system can be described as:</p>
<script type="math/tex; mode=display">
\left[
 \begin{matrix}
S(0)\\
\vdots \\
S(L+N-1)
  \end{matrix} 
\right]
\xrightarrow{IFFT}
\left[
 \begin{matrix}
x(0)\\
\vdots \\
x(L+N-1)
  \end{matrix} 
\right]
\xrightarrow{channel}
\left[
 \begin{matrix}
y(0)\\
\vdots \\
y(L+N-1)
  \end{matrix} 
\right]
\xrightarrow{FFT}
Y(k)=(\sum_{l=0}^{N-1}{h_{l}}e^{-j\frac{2\pi}{N}kl})S(k)+N(k)</script><p>$L$ is the length of original signal and $N​$ is the length of pilot sequences. Then we can build a linear model to gain the MVU estimator:</p>
<script type="math/tex; mode=display">
\left[
 \begin{matrix}
Y(P_0)\\
\vdots \\
Y(P_{N-1})
  \end{matrix} 
\right]=
\left[
 \begin{matrix}
S
  \end{matrix} 
\right]
\left[
 \begin{matrix}
h_o\\
\vdots \\
h_{L-1}
  \end{matrix} 
\right]
+ \mathbf{N}

\Rightarrow

\mathbf{Y}=\mathbf{H}\boldsymbol{\theta}+\mathbf{N}</script><p>$P$ is the pilot sequence and $\left[<br> \begin{matrix}<br>S<br>  \end{matrix}<br>\right]$ is the combination of $e^{-j\frac{2\pi}{N}kl}$ and $S(k)$.</p>
<hr>
<h1 id="Maximum-Likelihood-Estimation"><a href="#Maximum-Likelihood-Estimation" class="headerlink" title="Maximum Likelihood Estimation"></a>Maximum Likelihood Estimation</h1><h2 id="Scalar-parameter-1"><a href="#Scalar-parameter-1" class="headerlink" title="Scalar parameter"></a>Scalar parameter</h2><p>The MLE for a scalar parameter is defined to be <em>the value of $\theta​$ that maximizes $p(\mathbf{x};\theta)​$</em> for $\mathbf{x}​$ fixed, i.e., the value that maximizes the likelihood function.</p>
<p><em>The method defines a maximum likelihood estimate:</em></p>
<script type="math/tex; mode=display">
\hat{\theta}_{MLE}\in \{ \mathop{\arg\max}_{\theta}p(\mathbf{x};\theta) \}</script><p><em>if a maximum exists.</em></p>
<blockquote>
<p><strong>Relation to Bayesian inference:</strong><br>The maximum a posteriori estimate:</p>
<script type="math/tex; mode=display">
\hat{\theta}_{MAP}\in \{ \mathop{\arg\max}_{\theta}p(\theta\ |\ \mathbf{x}) \}</script><p>Bayes’ theorem:</p>
<script type="math/tex; mode=display">
p(\theta\ |\ x_1,x_2,...,x_n)=\frac{p(x_1, x_2,...,x_n\ | \ \theta)\ > p(\theta)}{p(x_1, x_2,...,x_n)}</script><p>when the prior $p(\theta)$ is a uniform distribution, <em>MAP</em> is the same as <em>MLE</em>.</p>
</blockquote>
<p><strong>Theorem 1 (Asymptotic Properties of the MLE):</strong></p>
<p>From the asymptotic distribution, the MLE is seen to be asymptotically unbiased and asymptotically attains the CRLB. It is therefore <strong><em>asymptotically efficient</em></strong>, and hence <strong><em>asymptotically optimal</em></strong>.</p>
<p><strong>Theorem 2 (Invariance Property of the MLE)</strong></p>
<p>Similar to “Cramer-Rao Lower Bound/ Scalar parameter · transformation”</p>
<p><em>The MLE of the parameter $\alpha = g(\theta)$, where the PDF $p(\mathbf{x}; \theta)$ is parameterized by $\theta$, is given by</em></p>
<script type="math/tex; mode=display">
\hat{\alpha}=g(\hat{\theta})</script><p><em>where $\hat{\theta}$ is the MLE of $\theta$.</em></p>
<h2 id="Numerical-determination"><a href="#Numerical-determination" class="headerlink" title="Numerical determination"></a>Numerical determination</h2><p>We attempt to solve the equation:</p>
<script type="math/tex; mode=display">
g(\theta)=\frac{\partial  \ln p(\mathbf{x}; \theta)}{\partial \theta}</script><p>Then, if $g(\theta)$ is approximately linear near $\theta_0$, we can approximate it by (<em>Taylor</em> series):</p>
<script type="math/tex; mode=display">
g(\theta)\approx g(\theta_0)+\frac{dg(\theta)}{d\theta}|_{\theta=\theta_0}(\theta-\theta_0)</script><p><img src="/assets/1560249863674.png" alt="1560249863674"></p>
<hr>
<h1 id="Least-Squares"><a href="#Least-Squares" class="headerlink" title="Least Squares"></a>Least Squares</h1><h2 id="Linear-Least-Squares"><a href="#Linear-Least-Squares" class="headerlink" title="Linear Least Squares"></a>Linear Least Squares</h2><p>In applying the linear LS approach for a scalar parameter we must assume that</p>
<script type="math/tex; mode=display">
s[n]=\theta h[n]</script><p>where $h[n]$ is a known sequence and $s[n]$ is a purely deterministic signal. Due to observation noise or model inaccuracies we observe a perturbed version of $s[n]$, which we denote by $x[n]$. The LS error criterion becomes</p>
<script type="math/tex; mode=display">
J(\theta)=\sum_{n=0}^{N-1}(x[n]-\theta h[n])^2.</script><p>A minimization is readily shown to produce the LSE</p>
<script type="math/tex; mode=display">
\hat{\theta}=\frac{\sum_{n=0}^{N-1}x[n]h[n]}{\sum_{n=0}^{N-1}h^2[n]}.</script><p>This equation is similar to $\frac{R(xy)}{R(yy)}$ in channel estimation.</p>
<font face="Segoe Print" color="#D87093" size="4">class note: Conclusion of estimation methods</font>

<ul>
<li>CRLB:<ul>
<li>Condition:<ul>
<li>know the PDF $p(\mathbf{x};\theta)$</li>
<li>the “regularity” condition $E[\frac{\partial \ \ln p(\mathbf{x};\theta)}{\partial \theta}]=0 \ \ \ \ for\ all\ \theta$</li>
</ul>
</li>
<li>Estimator: MVU</li>
</ul>
</li>
</ul>
<ul>
<li>Linear Model:<ul>
<li>Condition:<ul>
<li>know the PDF $p(\mathbf{x};\theta)​$</li>
<li>the distribution is gaussian</li>
<li>can use the form: $\mathbf x = \mathbf H \boldsymbol \theta + \mathbf w$</li>
</ul>
</li>
<li>Estimator: MVU</li>
</ul>
</li>
</ul>
<ul>
<li>MLE:<ul>
<li>Condition:<ul>
<li>know the PDF $p(\mathbf{x};\theta)$</li>
</ul>
</li>
<li>Estimator: <ul>
<li>if  $p(\mathbf{x};\theta)$ is gaussian, then MVU</li>
<li>if  $p(\mathbf{x};\theta)$ is not gaussian, then asymptotically MVU</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>LSE:<ul>
<li>Condition:<ul>
<li>none</li>
</ul>
</li>
<li>Estimator: <ul>
<li>if  $p(\mathbf{x};\theta)​$ is gaussian, then MVU</li>
<li>if  $p(\mathbf{x};\theta)$ is not gaussian, then whether it’s MVU is unclear because we don’t have statistics of $\mathbf{x}$ </li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h1 id="Statistical-detection-theory"><a href="#Statistical-detection-theory" class="headerlink" title="Statistical detection theory"></a>Statistical detection theory</h1><p>Our intention is to do a binary hypothesis testing</p>
<script type="math/tex; mode=display">
H_0\ or\  H_1?</script><ul>
<li>$P(H_0;H_0)​$ = prob(decide $H_0​$ when $H_0​$ is true) = prob of correct non-detection </li>
<li>$P(H_0;H_1)​$ = prob(decide $H_0​$ when $H_1​$ is true) = prob of missed detection = $P_M​$</li>
<li>$P(H_1;H_0)$ = prob(decide $H_1$ when $H_0$ is true) = prob of false alarm = $P_{FA}$ </li>
<li>$P(H_1;H_1)$ = prob(decide $H_1$ when $H_1$ is true) = prob of detection = $P_D$</li>
</ul>
<h2 id="Neyman-Pearson-Theorem"><a href="#Neyman-Pearson-Theorem" class="headerlink" title="Neyman-Pearson Theorem"></a>Neyman-Pearson Theorem</h2><p><strong>Condition: None</strong></p>
<p>To maximize $P_D$ for a given $P_{FA}=a $ , decide $H_1$ if:</p>
<script type="math/tex; mode=display">
L(x)=\frac{p(x;H_1)}{p(x;H_0)} > \gamma</script><p>where the threshold $\gamma​$ is found from:</p>
<script type="math/tex; mode=display">
P_{FA}=\int_{\{x:L(x)>\gamma \}} p(x;H_0)dx = \alpha</script><p>$L(x)​$ is the <em>likelihood ratio</em> , and comparing $L(x)​$ to a threshold is termed the <em>likelihood ratio test</em>.</p>
<h2 id="Minimum-Probability-of-Error"><a href="#Minimum-Probability-of-Error" class="headerlink" title="Minimum Probability of Error"></a>Minimum Probability of Error</h2><p><strong>Condition: Priors $P(H_0), P(H_1)$ </strong></p>
<p>Assume that we know the prior probability $P(H_0)$ and $P(H_1)$, our goal is to minimize the <em>probability of error $P_e $</em> :</p>
<script type="math/tex; mode=display">
P_e=P(H_0|H_1)P(H_1)+P(H_1|H_0)P(H_0)</script><p>It is a special case of <em>Bayes Risk</em>. So the estimator is to decide $H_{1}$ if:</p>
<script type="math/tex; mode=display">
\frac{p(x|H_1)}{p(x|H_0)}>\frac{P(H_0)}{P(H_1)}=\gamma</script><h2 id="Bayes-Risk"><a href="#Bayes-Risk" class="headerlink" title="Bayes Risk"></a>Bayes Risk</h2><p><strong>Condition: Priors $P(H_0), P(H_1)$  &amp; Costs $C_{ij}​$</strong></p>
<p>Associate with each of the four detection possibilities a cost. $C_{ij}​$ is the cost of deciding hypothesis $H_i​$ when hypothesis $H_j​$ is true.</p>
<script type="math/tex; mode=display">
Bayes\ risk: R=E(C)=\sum_{i=0}^{1}{\sum_{j=0}^{1}{C_{ij} P(H_i|H_j) P(H_j)}}</script><p>Under the assumption that $C_{10}&gt;C_{00}​$ , $C_{01}&gt;C_{11}​$ , the detector that minimizes the Bayes risk is to decide $H_{1}​$ if:</p>
<script type="math/tex; mode=display">
\frac{p(x|H_1)}{p(x|H_0)}>\frac{(C_{10}-C_{00})P(H_0)}{(C_{01}-C_{11})P(H_1)}=\gamma</script><h2 id="Minimax-Hypothesis-Testing"><a href="#Minimax-Hypothesis-Testing" class="headerlink" title="Minimax Hypothesis Testing"></a>Minimax Hypothesis Testing</h2><p><strong>Condition: Costs $C_{ij}$</strong></p>
<p>In general, no single decision rule minimizes the Bayes risk over all possible  $P(H_{i/j})\in [0, 1]​$. Our intention is to avoid possible huge risk so we try to minimize the maximum loss. </p>
<p>Minimax Hypothesis Testing Formulation:</p>
<script type="math/tex; mode=display">
\mathop{min}_{}(\mathop{max}_{P(H_{i/j})}(R))</script><p> <img src="/assets/cminmax.png" alt="1561529930748"></p>
<p>“a” is the minimum Bayesian risk when $P(H_{1}) \in [0,1]$. Once we assume that the prior probability $P(H_1)=P_{1g}$,   $P_{FA}$ and $P_M$ are known. and “a” will be reshaped as “b”, which may leads to huge risk when the exact $P_{1}$ is far different from $P_{1g}$. </p>
<p><strong>Minimax Hypothesis Testing</strong> tends to assume that $P(H_1)=P_{1g}^{*}$.</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/signal-processing/" rel="tag"># signal processing</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/04/20/Lecture-Note-Stanford-Computer-Vision-CS231n/" rel="next" title="Lecture Note: Stanford Computer Vision CS231n">
                <i class="fa fa-chevron-left"></i> Lecture Note: Stanford Computer Vision CS231n
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/05/12/微波技术与天线/" rel="prev" title="微波技术与天线">
                微波技术与天线 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Michael.Xiu</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/Michael-Xiu" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.zhihu.com/people/xiao-li-dou-dan/activities" target="_blank" title="Zhihu">
                      
                        <i class="fa fa-fw fa-paper-plane"></i>Zhihu</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:xiushj@mail2.sysu.edu.cn" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.facebook.com/shengjie.xiu.3" target="_blank" title="FB Page">
                      
                        <i class="fa fa-fw fa-facebook"></i>FB Page</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://instagram.com/michael.xiu" target="_blank" title="Instagram">
                      
                        <i class="fa fa-fw fa-instagram"></i>Instagram</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="skype:Shengjie Xiu?call|chat" target="_blank" title="Skype">
                      
                        <i class="fa fa-fw fa-skype"></i>Skype</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Random-Signal-Basic"><span class="nav-text">Random Signal Basic</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Review"><span class="nav-text">Review</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Probability-distribution"><span class="nav-text">Probability distribution</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Moment-and-Cumulant"><span class="nav-text">Moment and Cumulant</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Statistical-independence-and-irrelevance"><span class="nav-text">Statistical independence and irrelevance</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Characteristic-function"><span class="nav-text">Characteristic function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Distribution-model"><span class="nav-text">Distribution model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Easy-ones"><span class="nav-text">Easy ones</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gaussian-distribution"><span class="nav-text">Gaussian distribution</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#chi-2-distribution"><span class="nav-text">$\chi^2$ distribution</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Rayleigh-distribution"><span class="nav-text">Rayleigh distribution</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Stochastic-Process"><span class="nav-text">Stochastic Process</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#From-random-variable-to-stochastic-process"><span class="nav-text">From random variable to stochastic process</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Stochastic-process-definition"><span class="nav-text">Stochastic process definition</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Probability-distribution-1"><span class="nav-text">Probability distribution</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Numerical-characteristic"><span class="nav-text">Numerical characteristic</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Stationary-process-and-ergodicity"><span class="nav-text">Stationary process and ergodicity</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Strictly-stationary-process"><span class="nav-text">Strictly stationary process</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Weakly-stationary-process"><span class="nav-text">Weakly stationary process</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Ergodicity"><span class="nav-text">Ergodicity</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Power-spectrum-density-PSD"><span class="nav-text">Power spectrum density (PSD)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#PSD-definition"><span class="nav-text">PSD definition</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PSD-and-autocorrelation"><span class="nav-text">PSD and autocorrelation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gaussian-process-and-white-noise"><span class="nav-text">Gaussian process and white noise</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Gaussian-process"><span class="nav-text">Gaussian process</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#White-noise"><span class="nav-text">White noise</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#System-response"><span class="nav-text">System response</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#PDF"><span class="nav-text">PDF</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-gaussian-Input"><span class="nav-text">1. gaussian Input</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-non-gaussian-input"><span class="nav-text">2. non-gaussian input</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Expectation-and-autocorrelation"><span class="nav-text">Expectation and autocorrelation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PSD"><span class="nav-text">PSD</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction-of-Estimation-Theory"><span class="nav-text">Introduction of Estimation Theory</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Estimation-in-signal-processing"><span class="nav-text">Estimation in signal processing</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-mathematical-estimation-problem"><span class="nav-text">The mathematical estimation problem</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Minimum-Variance-Unbiased-Estimation"><span class="nav-text">Minimum Variance Unbiased Estimation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Unbiased-estimation"><span class="nav-text">Unbiased estimation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Minimum-Variance-Criterion"><span class="nav-text">Minimum Variance Criterion</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Cramer-Rao-Lower-Bound"><span class="nav-text">Cramer-Rao Lower Bound</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Efficiency"><span class="nav-text">Efficiency</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Scalar-parameter"><span class="nav-text">Scalar parameter</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Scalar-parameter-·-transformation"><span class="nav-text">Scalar parameter · transformation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Vector-parameter"><span class="nav-text">Vector parameter</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Vector-parameter-·-transformation"><span class="nav-text">Vector parameter · transformation</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Linear-Model"><span class="nav-text">Linear Model</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Linear-model-·-specific"><span class="nav-text">Linear model · specific</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Linear-model-·-general"><span class="nav-text">Linear model · general</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Maximum-Likelihood-Estimation"><span class="nav-text">Maximum Likelihood Estimation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Scalar-parameter-1"><span class="nav-text">Scalar parameter</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Numerical-determination"><span class="nav-text">Numerical determination</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Least-Squares"><span class="nav-text">Least Squares</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Linear-Least-Squares"><span class="nav-text">Linear Least Squares</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Statistical-detection-theory"><span class="nav-text">Statistical detection theory</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Neyman-Pearson-Theorem"><span class="nav-text">Neyman-Pearson Theorem</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Minimum-Probability-of-Error"><span class="nav-text">Minimum Probability of Error</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Bayes-Risk"><span class="nav-text">Bayes Risk</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Minimax-Hypothesis-Testing"><span class="nav-text">Minimax Hypothesis Testing</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Michael Xiu</span>

  
</div>









        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
